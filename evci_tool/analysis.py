# AUTOGENERATED! DO NOT EDIT! File to edit: ../03_analysis.ipynb.

# %% auto 0
__all__ = ['run_episode', 'analyze_sites']

# %% ../03_analysis.ipynb 4
import numpy as np
import pandas as pd
import geopandas as gpd

import shapely

import os
from tqdm import tqdm

import matplotlib.pyplot as plt

from scipy.cluster.vq import kmeans2, whiten
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster.hierarchy import fcluster

from .config import *
from .model import *

import warnings
warnings.filterwarnings("ignore")

# %% ../03_analysis.ipynb 5
def run_episode(m,s,t,g,ui_inputs,s_df,txt,OUTPUT_PATH,corridor):
    "This function runs a full episode of analysis on a set of sites."
    
    print('\n' + txt.capitalize() + ' Analysis')
    print('________________\n')
    total = s_df.shape[0]
    
    #s_df = s_df[s_df['year 1'] == 1]
    #s_df = s_df.reset_index(drop=True)
    
    Nc = s_df.shape[0]
    print(f'Number of sites: {Nc}/{total}')

    #@title Compute scores

    backoff_factor = ui_inputs['backoff_factor']

    u_df = run_analysis(m,s,t,g,ui_inputs,s_df,backoff_factor=backoff_factor)

    print(f'Total capex charges = INR Cr {sum(u_df.capex)/1e7:.2f}')
    print(f'Total opex charges = INR Cr {sum(u_df.opex)/1e7:.2f}')
    print(f'Total Margin = INR Cr {sum(u_df.margin)/1e7:.2f}')        

    #@title Prepare data
    s_u_df = s_df.copy()

    s_u_df['utilization'] = u_df.utilization
    s_u_df['unserviced'] = u_df.unserviced
    s_u_df['capex'] = u_df.capex
    s_u_df['opex'] = u_df.opex
    s_u_df['margin'] = u_df.margin
    s_u_df['max vehicles'] = u_df['max vehicles']
    s_u_df['estimated vehicles'] = u_df['estimated vehicles']

    #@title Save initial analysis to Excel
    output_df = s_u_df.copy()
    output_df.drop('geometry', axis=1, inplace=True)
    
    # Save output dataframe as both xlsx and json
    output_df.to_excel(OUTPUT_PATH + '/' + txt + '_evci_analysis.xlsx')
    output_df.to_json(OUTPUT_PATH + '/' + txt + '_evci_analysis.json', orient='records')
    
    return s_u_df

# %% ../03_analysis.ipynb 7
def analyze_sites(corridor:str, ui_inputs):
    "The function analyzes sites specified as part of a corridor."

    #@title Read data from excel sheets
    model,site,traffic,grid, INPUT_PATH, OUTPUT_PATH = setup_and_read_data(corridor)
    
    #set variables for clustering etc from the UI
    cluster = ui_inputs['cluster']
    cluster_th = ui_inputs['cluster_th']
    plot_dendrogram = ui_inputs['plot_dendrogram']
    use_defaults = ui_inputs['use_defaults']

    #check if mandatory worksheets in xlsx files are available
    avail = data_availability_check(model,site,traffic,grid)
    assert len(avail) == 0, f"{avail} sheets missing from the xlsx. Please try again." 
    
    #check if any missingness
    missing = data_integrity_check(model,site,traffic,grid)
    #assert len(missing) > 0, f"{missing} sheets contain missing data." 

    #if missing values found, defaults shall be assumed for debug purposes. This is not for production version
    if use_defaults and len(missing) > 0:
        site['sites']['Traffic congestion'] = 1
        site['sites']['Year for Site recommendation'] = 1
        site['sites']['Hoarding/Kiosk (1 is yes & 0 is no)'] = 1
        site['sites']['Hoarding margin'] = 270000    
    
    #@title Read required data sheets only
    #df = gpd.read_file(INPUT_PATH + '/shape_files/' + corridor + '.shp')

    data = site['sites']
    data['Name'] = data['Name']
    data['Latitude'] = pd.to_numeric(data['Latitude'])
    data['Longitude'] = pd.to_numeric(data['Longitude'])
    data['geometry'] = [shapely.geometry.Point(xy) for xy in 
                        zip(data['Longitude'], data['Latitude'])]

    data_df = {}

    data_df = gpd.GeoDataFrame(data, geometry=data['geometry'])

    s_df = pd.DataFrame(columns=['Name',
                                'Latitude', 'Longitude',
                                'Traffic congestion',
                                'year 1',
                                'kiosk hoarding',
                                'hoarding margin',
                                'geometry'])

    s_df = s_df.reset_index(drop=True)

    for i in range(data_df.shape[0]):
        s_df.loc[i] = [
           data_df.loc[i].Name, 
           data_df.loc[i].Latitude, 
           data_df.loc[i].Longitude, 
           data_df.loc[i]['Traffic congestion'],
           data_df.loc[i]['Year for Site recommendation'],
           data_df.loc[i]['Hoarding/Kiosk (1 is yes & 0 is no)'],
           data_df.loc[i]['Hoarding margin'],
           data_df.loc[i].geometry
        ] 

    s_u_df = run_episode(model,site,traffic,grid,ui_inputs,s_df,'initial',OUTPUT_PATH, corridor)

    #@title Threshold and cluster
    if cluster:
        #clustering_candidates = s_u_df[(s_u_df.utilization <= cluster_th) & (s_u_df['year 1'] == 1)]
        clustering_candidates = s_u_df[s_u_df.utilization <= cluster_th]
        print('candidates for clustering: ', clustering_candidates.shape[0])
        points = np.array((clustering_candidates.apply(lambda x: list([x['Latitude'], x['Longitude']]),axis=1)).tolist())
        Z = linkage (points, method='complete', metric='euclidean');
        if plot_dendrogram:
            plt.figure(figsize=(14,8))
            dendrogram(Z);
        max_d = 0.01
        clusters = fcluster(Z, t=max_d, criterion='distance')
        clustered_candidates = gpd.GeoDataFrame(clustering_candidates)
        #base = grid_df.plot(color='none', alpha=0.2, edgecolor='black', figsize=(8,8))
        #clustered_candidates.plot(ax=base, column=clusters, legend=True)

    #@title Build final list of sites
    confirmed_sites = s_u_df[s_u_df.utilization > cluster_th]
    print(f'confirmed sites with utilization > {int(cluster_th*100)}%: {confirmed_sites.shape[0]}')
    if cluster:
        val, ind = np.unique (clusters, return_index=True)
        clustered_sites = clustered_candidates.reset_index(drop=True)
        clustered_sites = clustered_sites.iloc[clustered_sites.index.isin(ind)]
        final_list_of_sites = pd.concat([confirmed_sites, clustered_sites], axis=0)
    else:
        final_list_of_sites = confirmed_sites.copy()

    if cluster:
        print('final list: ', final_list_of_sites.shape[0])
        s_df = final_list_of_sites.copy()
        s_df = s_df.reset_index(drop=True)
        
        s_u_df = run_episode(model,site,traffic,grid,ui_inputs,s_df,'clustered',OUTPUT_PATH, corridor)
    
    return s_u_df
